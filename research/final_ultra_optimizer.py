#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Final Ultra Optimizer - é›†æˆè®­ç»ƒ/æµ‹è¯•åˆ†å‰²çš„è¶…é«˜æ€§èƒ½ä¼˜åŒ–å™¨
è§£å†³æ•°æ®æ³„éœ²é—®é¢˜ï¼Œæä¾›çœŸå®å¯ä¿¡çš„OOSæ”¶ç›Šç‡
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import warnings
import time
from datetime import datetime, timedelta
import json
import os

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

@dataclass
class BacktestParams:
    """å›æµ‹å‚æ•°"""
    buy_threshold: float
    stop_loss: float
    take_profit: float

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    symbol: str
    best_params: BacktestParams
    train_return: float
    test_return: float
    train_days: int
    test_days: int
    train_period: Tuple[pd.Timestamp, pd.Timestamp]
    test_period: Tuple[pd.Timestamp, pd.Timestamp]
    consistency_ratio: float

class FinalUltraOptimizer:
    """
    æœ€ç»ˆè¶…é«˜æ€§èƒ½ä¼˜åŒ–å™¨
    é›†æˆè®­ç»ƒ/æµ‹è¯•åˆ†å‰²ï¼Œè§£å†³æ•°æ®æ³„éœ²é—®é¢˜
    """
    
    def __init__(self, data: pd.DataFrame, test_days: int = 90):
        """
        åˆå§‹åŒ–æœ€ç»ˆä¼˜åŒ–å™¨
        
        Args:
            data: å®Œæ•´å¸‚åœºæ•°æ®
            test_days: æµ‹è¯•æœŸå¤©æ•°ï¼ˆé»˜è®¤90å¤©ï¼‰
        """
        self.data = data.copy()
        self.test_days = test_days
        self.symbols = data['symbol'].unique()
        self.prepare_data_with_split()
        
        logger.info(f"ğŸš€ Final Ultra Optimizer initialized with {len(self.data)} records for {len(self.symbols)} symbols")
        logger.info(f"ğŸ“… Test period: {test_days} days")
    
    def prepare_data_with_split(self):
        """é¢„å¤„ç†æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒ/æµ‹è¯•åˆ†å‰²"""
        logger.info("ğŸ”„ Preparing data with train/test split...")
        
        self.train_data = {}
        self.test_data = {}
        
        for symbol in self.symbols:
            df = self.data[self.data['symbol'] == symbol].copy()
            
            if len(df) == 0:
                continue
            
            # ç¡®ä¿æ—¶é—´æ ¼å¼
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            # æ—¶é—´åˆ†å‰²
            latest_time = df['timestamp'].max()
            split_time = latest_time - pd.Timedelta(days=self.test_days)
            
            train_df = df[df['timestamp'] < split_time].copy()
            test_df = df[df['timestamp'] >= split_time].copy()
            
            if len(train_df) < 500 or len(test_df) < 50:
                logger.warning(f"Insufficient data for {symbol}: train={len(train_df)}, test={len(test_df)}")
                continue
            
            # é¢„å¤„ç†è®­ç»ƒæ•°æ®
            train_processed = self._process_data_for_symbol(train_df)
            test_processed = self._process_data_for_symbol(test_df)
            
            self.train_data[symbol] = train_processed
            self.test_data[symbol] = test_processed
            
            logger.info(f"âœ… {symbol}: Train {len(train_df)} records, Test {len(test_df)} records")
        
        logger.info(f"âœ… Data preparation complete for {len(self.train_data)} symbols")
    
    def _process_data_for_symbol(self, df: pd.DataFrame) -> Dict:
        """å¤„ç†å•ä¸ªå¸ç§çš„æ•°æ®ä¸ºå‘é‡åŒ–æ ¼å¼"""
        # æå–æ—¥æœŸå’Œè®¡ç®—æ—¥å¼€ç›˜ä»·
        df['date'] = df['timestamp'].dt.tz_localize('UTC').dt.date
        df['hour_in_day'] = df.groupby('date').cumcount()
        
        # è®¡ç®—æ¯æ—¥å¼€ç›˜ä»·
        daily_opens = df.groupby('date')['open'].first()
        df['daily_open'] = df['date'].map(daily_opens)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        return {
            'dates': df['date'].values,
            'hours_in_day': df['hour_in_day'].values,
            'opens': df['open'].values,
            'highs': df['high'].values,
            'lows': df['low'].values,
            'closes': df['close'].values,
            'daily_opens': df['daily_open'].values,
            'timestamps': df['timestamp'].values,
            'unique_dates': df['date'].unique(),
            'period_start': df['timestamp'].min(),
            'period_end': df['timestamp'].max()
        }
    
    def ultra_fast_backtest(self, symbol: str, b: float, l: float, p: float, use_test_data: bool = False) -> float:
        """
        è¶…å¿«é€Ÿå›æµ‹
        
        Args:
            symbol: å¸ç§
            b, l, p: å‚æ•°
            use_test_data: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ•°æ®
            
        Returns:
            æ€»æ”¶ç›Šç‡
        """
        data_source = self.test_data if use_test_data else self.train_data
        
        if symbol not in data_source:
            return 0.0
        
        data = data_source[symbol]
        dates = data['dates']
        hours_in_day = data['hours_in_day']
        lows = data['lows']
        highs = data['highs']
        closes = data['closes']
        daily_opens = data['daily_opens']
        unique_dates = data['unique_dates']
        
        total_return = 1.0
        
        # å‘é‡åŒ–å¤„ç†æ¯ä¸ªäº¤æ˜“æ—¥
        for date in unique_dates:
            day_mask = dates == date
            day_hours = hours_in_day[day_mask]
            day_lows = lows[day_mask]
            day_highs = highs[day_mask]
            day_closes = closes[day_mask]
            day_open = daily_opens[day_mask][0]
            
            if len(day_hours) <= 1:
                continue
            
            # è®¡ç®—å…³é”®ä»·ä½
            B = day_open * (1 - b)
            SL = day_open * (1 - l)
            TP = day_open * (1 + p)
            
            # å‘é‡åŒ–å¯»æ‰¾ä¹°å…¥ç‚¹
            after_open_mask = day_hours > 0
            if not after_open_mask.any():
                continue
            
            after_open_lows = day_lows[after_open_mask]
            buy_signals = after_open_lows <= B
            
            if not buy_signals.any():
                continue
            
            # æ‰¾åˆ°é¦–æ¬¡ä¹°å…¥ç‚¹
            buy_idx = np.argmax(buy_signals)
            entry_price = B
            
            # æ£€æŸ¥ä¹°å…¥åçš„æ•°æ®
            post_buy_start = buy_idx + 1
            if post_buy_start >= len(after_open_lows):
                exit_price = day_closes[-1]
            else:
                # å‘é‡åŒ–æ£€æŸ¥æ­¢æŸæ­¢ç›ˆ
                post_lows = day_lows[after_open_mask][post_buy_start:]
                post_highs = day_highs[after_open_mask][post_buy_start:]
                
                sl_hits = post_lows <= SL
                tp_hits = post_highs >= TP
                
                sl_indices = np.where(sl_hits)[0]
                tp_indices = np.where(tp_hits)[0]
                
                if len(sl_indices) > 0 and len(tp_indices) > 0:
                    if sl_indices[0] <= tp_indices[0]:
                        exit_price = SL
                    else:
                        exit_price = TP
                elif len(sl_indices) > 0:
                    exit_price = SL
                elif len(tp_indices) > 0:
                    exit_price = TP
                else:
                    exit_price = day_closes[-1]
            
            # ç´¯è®¡æ”¶ç›Š
            trade_return = exit_price / entry_price
            total_return *= trade_return
        
        return total_return - 1
    
    def optimize_single_symbol_with_split(self, symbol: str, param_ranges: Dict[str, np.ndarray]) -> OptimizationResult:
        """
        å•å¸ç§ä¼˜åŒ–ï¼ˆå¸¦è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ï¼‰
        
        Args:
            symbol: å¸ç§
            param_ranges: å‚æ•°èŒƒå›´
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        if symbol not in self.train_data or symbol not in self.test_data:
            return None
        
        logger.info(f"ğŸ” Optimizing {symbol} with train/test split...")
        
        b_range = param_ranges['buy_threshold']
        l_range = param_ranges['stop_loss']
        p_range = param_ranges['take_profit']
        
        best_params = None
        best_train_return = -float('inf')
        total_tests = 0
        
        # åœ¨è®­ç»ƒæ•°æ®ä¸Šå¯»æ‰¾æœ€ä¼˜å‚æ•°
        for b in b_range:
            for l in l_range:
                if l < b * 0.5:
                    continue
                for p in p_range:
                    train_return = self.ultra_fast_backtest(symbol, b, l, p, use_test_data=False)
                    total_tests += 1
                    
                    if train_return > best_train_return:
                        best_train_return = train_return
                        best_params = BacktestParams(
                            buy_threshold=b,
                            stop_loss=l,
                            take_profit=p
                        )
        
        if best_params is None:
            logger.error(f"No valid parameters found for {symbol}")
            return None
        
        # åœ¨æµ‹è¯•æ•°æ®ä¸ŠéªŒè¯æœ€ä¼˜å‚æ•°
        test_return = self.ultra_fast_backtest(
            symbol, 
            best_params.buy_threshold, 
            best_params.stop_loss, 
            best_params.take_profit, 
            use_test_data=True
        )
        
        # è®¡ç®—ä¸€è‡´æ€§æ¯”ç‡
        consistency_ratio = test_return / best_train_return if best_train_return != 0 else 0
        
        # è·å–æ—¶é—´æ®µä¿¡æ¯
        train_data = self.train_data[symbol]
        test_data = self.test_data[symbol]
        
        train_days = (train_data['period_end'] - train_data['period_start']).days + 1
        test_days = (test_data['period_end'] - test_data['period_start']).days + 1
        
        result = OptimizationResult(
            symbol=symbol,
            best_params=best_params,
            train_return=best_train_return,
            test_return=test_return,
            train_days=train_days,
            test_days=test_days,
            train_period=(train_data['period_start'], train_data['period_end']),
            test_period=(test_data['period_start'], test_data['period_end']),
            consistency_ratio=consistency_ratio
        )
        
        logger.info(f"âœ… {symbol} - Train: {best_train_return:.2%} ({train_days}d), Test: {test_return:.2%} ({test_days}d), Consistency: {consistency_ratio:.2f}")
        
        return result
    
    def batch_optimize_with_split(self, symbols: Optional[List[str]] = None, 
                                param_ranges: Optional[Dict[str, np.ndarray]] = None) -> Dict[str, OptimizationResult]:
        """
        æ‰¹é‡ä¼˜åŒ–ï¼ˆå¸¦è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ï¼‰
        
        Args:
            symbols: å¸ç§åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            param_ranges: å‚æ•°èŒƒå›´
            
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        if symbols is None:
            symbols = list(self.train_data.keys())
        
        if param_ranges is None:
            param_ranges = self.create_default_param_ranges()
        
        logger.info(f"ğŸš€ Starting batch optimization with train/test split for {len(symbols)} symbols")
        
        start_time = time.time()
        results = {}
        
        for symbol in symbols:
            try:
                result = self.optimize_single_symbol_with_split(symbol, param_ranges)
                if result:
                    results[symbol] = result
                else:
                    logger.warning(f"Failed to optimize {symbol}")
            except Exception as e:
                logger.error(f"Error optimizing {symbol}: {e}")
                continue
        
        duration = time.time() - start_time
        successful = len(results)
        
        logger.info(f"ğŸ‰ Batch optimization completed in {duration:.1f} seconds")
        logger.info(f"âœ… {successful}/{len(symbols)} symbols optimized successfully")
        
        return results
    
    def create_default_param_ranges(self) -> Dict[str, np.ndarray]:
        """åˆ›å»ºé»˜è®¤å‚æ•°èŒƒå›´"""
        return {
            'buy_threshold': np.array([0.005, 0.0075, 0.01, 0.0125, 0.015, 0.02]),
            'stop_loss': np.array([0.005, 0.0075, 0.01, 0.0125, 0.015, 0.02]),
            'take_profit': np.array([0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05])
        }
    
    def save_results(self, results: Dict[str, OptimizationResult], filename: Optional[str] = None) -> str:
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"final_ultra_optimization_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_results = {}
        
        for symbol, result in results.items():
            serializable_results[symbol] = {
                'symbol': result.symbol,
                'best_parameters': {
                    'buy_threshold': result.best_params.buy_threshold,
                    'stop_loss': result.best_params.stop_loss,
                    'take_profit': result.best_params.take_profit,
                    'buy_threshold_pct': result.best_params.buy_threshold * 100,
                    'stop_loss_pct': result.best_params.stop_loss * 100,
                    'take_profit_pct': result.best_params.take_profit * 100
                },
                'train_performance': {
                    'return': result.train_return,
                    'days': result.train_days,
                    'period_start': result.train_period[0].isoformat(),
                    'period_end': result.train_period[1].isoformat(),
                    'annualized_return': (1 + result.train_return) ** (365 / result.train_days) - 1 if result.train_days > 0 else 0
                },
                'test_performance': {
                    'return': result.test_return,
                    'days': result.test_days,
                    'period_start': result.test_period[0].isoformat(),
                    'period_end': result.test_period[1].isoformat(),
                    'annualized_return': (1 + result.test_return) ** (365 / result.test_days) - 1 if result.test_days > 0 else 0
                },
                'consistency_ratio': result.consistency_ratio
            }
        
        # æ·»åŠ æ±‡æ€»ç»Ÿè®¡
        if results:
            test_returns = [r.test_return for r in results.values()]
            train_returns = [r.train_return for r in results.values()]
            consistency_ratios = [r.consistency_ratio for r in results.values()]
            
            summary = {
                'optimization_info': {
                    'timestamp': datetime.now().isoformat(),
                    'total_symbols': len(results),
                    'test_days': self.test_days,
                    'method': 'Final Ultra Optimizer with Train/Test Split'
                },
                'summary_statistics': {
                    'train_performance': {
                        'average_return': float(np.mean(train_returns)),
                        'median_return': float(np.median(train_returns)),
                        'best_return': float(np.max(train_returns)),
                        'worst_return': float(np.min(train_returns))
                    },
                    'test_performance': {
                        'average_return': float(np.mean(test_returns)),
                        'median_return': float(np.median(test_returns)),
                        'best_return': float(np.max(test_returns)),
                        'worst_return': float(np.min(test_returns))
                    },
                    'consistency_analysis': {
                        'average_consistency': float(np.mean(consistency_ratios)),
                        'positive_test_returns': int(sum(1 for r in test_returns if r > 0))
                    }
                },
                'detailed_results': serializable_results
            }
        else:
            summary = {'detailed_results': serializable_results}
        
        # ä¿å­˜æ–‡ä»¶
        try:
            project_root = os.path.dirname(os.path.dirname(__file__))
            filepath = os.path.join(project_root, 'data', filename)
            
            with open(filepath, 'w') as f:
                json.dump(summary, f, indent=2)
            
            logger.info(f"ğŸ’¾ Results saved to {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return ""

def print_final_results(results: Dict[str, OptimizationResult], top_n: int = 10):
    """æ‰“å°æœ€ç»ˆä¼˜åŒ–ç»“æœ"""
    if not results:
        print("No results to display")
        return
    
    # æŒ‰æµ‹è¯•æ”¶ç›Šæ’åº
    sorted_results = sorted(results.values(), key=lambda x: x.test_return, reverse=True)
    
    print("\n" + "="*80)
    print("ğŸ¯ FINAL ULTRA OPTIMIZER RESULTS (With Train/Test Split)")
    print("="*80)
    
    # ç»Ÿè®¡ä¿¡æ¯
    test_returns = [r.test_return for r in results.values()]
    train_returns = [r.train_return for r in results.values()]
    
    print(f"ğŸ“Š SUMMARY STATISTICS:")
    print(f"   Total symbols: {len(results)}")
    print(f"   Average test return: {np.mean(test_returns):.2%}")
    print(f"   Best test return: {np.max(test_returns):.2%}")
    print(f"   Positive test returns: {sum(1 for r in test_returns if r > 0)}/{len(test_returns)}")
    
    print(f"\nğŸ† TOP {min(top_n, len(sorted_results))} TEST PERFORMERS")
    print("="*80)
    print(f"{'Rank':<4} {'Symbol':<12} {'Train':<8} {'Test':<8} {'Ratio':<6} {'b%':<5} {'l%':<5} {'p%':<5} {'Test Days'}")
    print("-"*80)
    
    for i, result in enumerate(sorted_results[:top_n], 1):
        print(f"{i:<4} {result.symbol:<12} {result.train_return:>6.1%} {result.test_return:>6.1%} "
              f"{result.consistency_ratio:>5.2f} {result.best_params.buy_threshold*100:>4.1f} "
              f"{result.best_params.stop_loss*100:>4.1f} {result.best_params.take_profit*100:>4.1f} "
              f"{result.test_days:>8}")
    
    print(f"\nğŸ’¡ KEY INSIGHTS:")
    print(f"   - Test returns are based on out-of-sample data (last {sorted_results[0].test_days if sorted_results else 90} days)")
    print(f"   - These are realistic performance expectations")
    print(f"   - No data leakage - parameters optimized on historical data only")
